{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import bids\n",
    "from bids import BIDSLayout, BIDSValidator\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.datasets import load_mni152_template, load_mni152_brain_mask\n",
    "from nilearn.glm.first_level.hemodynamic_models import compute_regressor\n",
    "import nibabel as nib\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from scipy.stats import ttest_1samp, ttest_ind, t, sem\n",
    "import hbayesdm.models\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.config.set_option('extension_initial_dot', True)\n",
    "\n",
    "args = {\n",
    "    'root': '/data2/project_model_based_fmri/ds000005/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2pindex(array, p_value=0.05, flatten=False):\n",
    "    confidence = 1 - p_value\n",
    "    flattened_array = array.flatten()\n",
    "    \n",
    "    n = len(flattened_array)\n",
    "    m = np.mean(flattened_array)\n",
    "    std_err = sem(flattened_array)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    end = m + h\n",
    "    \n",
    "    ret = (flattened_array >= end) if flatten is True else (array >= end)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_masking(mask_path, p_value, zoom, smoothing_fwhm, interpolation_func, standardize=False, flatten=False):\n",
    "    if mask_path is None:\n",
    "        assert (mask_path is None)\n",
    "    elif type(mask_path) is str:\n",
    "        mask_files = [mask_path]\n",
    "    else:\n",
    "        if type(mask_path) is not type(Path()):\n",
    "            mask_path = Path(mask_path)\n",
    "        mask_files = [file for file in mask_path.glob('*.nii.gz')]\n",
    "\n",
    "    mni152_mask = load_mni152_brain_mask()\n",
    "\n",
    "    m = array2pindex(nib.load(mask_files[0]).get_fdata(), p_value, flatten)\n",
    "\n",
    "    if len(mask_files) == 1:\n",
    "        return m\n",
    "    else:\n",
    "        for i in range(1, len(mask_files)):\n",
    "            m |= array2pindex(nib.load(mask_files[i]).get_fdata(), p_value, flatten)\n",
    "    \n",
    "    m = block_reduce(m, zoom, interpolation_func)\n",
    "    m = 1 * (m > 0)\n",
    "\n",
    "    m_true = np.array([i for i, v in enumerate(m.flatten()) if v != 0])\n",
    "    masked_data = nib.Nifti1Image(m, affine=mni152_mask.affine)\n",
    "    masker = NiftiMasker(mask_img=masked_data,\n",
    "                         standardize=standardize,\n",
    "                         smoothing_fwhm=smoothing_fwhm)\n",
    "\n",
    "    return masked_data, masker, m_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(params):\n",
    "    image_filepath, confounds, motion_confounds, masker, masked_data, num = params\n",
    "    \n",
    "    if confounds is not None:\n",
    "        confounds = pd.read_table(confounds, sep='\\t')\n",
    "        confounds = confounds[motion_confounds]\n",
    "        confounds = confounds.to_numpy()\n",
    "\n",
    "    fmri_masked = resample_to_img(image_filepath, masked_data)\n",
    "    fmri_masked = masker.fit_transform(fmri_masked, confounds=confounds)\n",
    "    \n",
    "    return fmri_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bids_preprocess(root,\n",
    "                    save_path=None,\n",
    "                    save=True,\n",
    "                    single_file=False,\n",
    "                    zoom=(1, 1, 1),\n",
    "                    smoothing_fwhm=6,\n",
    "                    interpolation_func=np.mean,\n",
    "                    motion_confounds=['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z'],\n",
    "                    p_value=0.05,\n",
    "                    task_name='task-zero',\n",
    "                    standardize=False,\n",
    "                    ncore=os.cpu_count()):\n",
    "    \n",
    "    print('...loading and validating BIDS')\n",
    "    layout = BIDSLayout(root, derivatives=True)\n",
    "    nii_layout = layout.derivatives['fMRIPrep'].get(return_type='file', suffix='bold', extension='nii.gz')\n",
    "    reg_layout = layout.derivatives['fMRIPrep'].get(return_type='file', suffix='regressors', extension='tsv')\n",
    "    \n",
    "    n_subject = len(layout.get_subjects())\n",
    "    n_session = len(layout.get_session())\n",
    "    n_run = len(layout.get_run())\n",
    "    print('...done!')\n",
    "    \n",
    "    root = Path(root)\n",
    "    print(layout.derivatives)\n",
    "    mask_path = Path(layout.derivatives['fMRIPrep'].root) / 'mask'\n",
    "    print(mask_path)\n",
    "\n",
    "    print('...make custom mask file')\n",
    "    masked_data, masker, m_true = custom_masking(mask_path, p_value, zoom, smoothing_fwhm, interpolation_func, standardize)\n",
    "    print(masked_data.get_fdata().shape)\n",
    "    print(m_true.shape)\n",
    "    print('...preprocessing')\n",
    "    \n",
    "    print('...image processing using %s cores' % ncore)\n",
    "    params = [[z[0], z[1], motion_confounds, masker, masked_data, i]\n",
    "            for i, z in enumerate(zip(nii_layout, reg_layout))]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        X = np.array(list(executor.map(image_preprocess, params)))\n",
    "        \n",
    "        if n_session != 0:\n",
    "            X = X.reshape(n_subject, n_session, n_run, -1, m_true.shape[0])\n",
    "        else:\n",
    "            X = X.reshape(n_subject, n_run, -1, m_true.shape[0])\n",
    "    print('...done!')\n",
    "\n",
    "    if save:\n",
    "        if save_path is None:\n",
    "            sp = Path(layout.derivatives['fMRIPrep'].root) / 'data'\n",
    "        else:\n",
    "            sp = Path(save_path)\n",
    "            \n",
    "        if not sp.exists():\n",
    "            sp.mkdir()\n",
    "        \n",
    "        if single_file:\n",
    "            np.save(sp / 'X.npy', X)\n",
    "        else:\n",
    "            for i in range(X.shape[0]):\n",
    "                np.save(sp / f'X_{i+1}.npy', X[i])\n",
    "        np.save(sp / 'restore_map.npy', m_true)\n",
    "    \n",
    "    return X, masked_data, m_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loading and validating BIDS\n",
      "...done!\n",
      "{'fMRIPrep': BIDS Layout: .../ds000005/derivatives/fmriprep | Subjects: 16 | Sessions: 0 | Runs: 48}\n",
      "/data2/project_model_based_fmri/ds000005/derivatives/fmriprep/mask\n",
      "...make custom mask file\n",
      "(46, 55, 46)\n",
      "(6810,)\n",
      "...preprocessing\n",
      "...image processing using 88 cores\n",
      "...done!\n",
      "time elapsed: 142.01521801948547\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "X, masked_data, m_true = bids_preprocess(args['root'], single_file=True, zoom=(2, 2, 2), ncore=os.cpu_count())\n",
    "e = time.time()\n",
    "print('time elapsed:', e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 3, 240, 6810), (6810,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, m_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events(df_events_list, df_events_info):\n",
    "    new_df_list = df_events_list.copy()\n",
    "    for i in range(len(new_df_list)):\n",
    "        new_df_list[i]['run'] = df_events_info[i]['run']\n",
    "        new_df_list[i]['subjID'] = df_events_info[i]['subject']\n",
    "        new_df_list[i]['gamble'] = new_df_list[i]['respcat'].apply(lambda x: 1 if x == 1 else 0)\n",
    "        new_df_list[i]['cert'] = 0 # certain..?\n",
    "        \n",
    "    return new_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_modulation(df_events_list, latent_params):\n",
    "    new_df_list = df_events_list.copy()\n",
    "    for i in range(len(new_df_list)):\n",
    "        idx = new_df_list[i].iloc[0]['subjID']\n",
    "        new_df_list[i]['rho'] = latent_params.loc[idx]['rho']\n",
    "        new_df_list[i]['lambda'] = latent_params.loc[idx]['lambda']\n",
    "        new_df_list[i]['modulation'] = \\\n",
    "            (new_df_list[i]['gain'] ** new_df_list[i]['rho']) \\\n",
    "            - (new_df_list[i]['lambda'] * (new_df_list[i]['loss'] ** new_df_list[i]['rho']))\n",
    "        \n",
    "    return new_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [preprocess_events, calculate_modulation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_events(root, funcs, dm_model,\n",
    "                   hrf_model='glover',\n",
    "                   save_path=None,\n",
    "                   save=True,\n",
    "                   single_file=True,\n",
    "                   ncore=os.cpu_count()):\n",
    "    \n",
    "    layout = BIDSLayout(root, derivatives=True)\n",
    "    t_r = layout.get_tr()\n",
    "    events = layout.get(suffix='events', extension='tsv')\n",
    "    image_sample = nib.load(\n",
    "        layout.derivatives['fMRIPrep'].get(\n",
    "            return_type='file',\n",
    "            suffix='bold',\n",
    "            extension='nii.gz')[0]\n",
    "    )\n",
    "    n_scans = image_sample.shape[-1]\n",
    "    \n",
    "    df_events_list = [event.get_df() for event in events]\n",
    "    df_events_info = [event.get_entities() for event in events]\n",
    "    \n",
    "    if len(df_events_list) != len(df_events_info):\n",
    "        assert()\n",
    "    \n",
    "    df_events_list = funcs[0](df_events_list, df_events_info)\n",
    "        \n",
    "    dm_model = getattr(hbayesdm.models, dm_model)(\n",
    "        data=pd.concat(df_events_list), ncore=ncore)\n",
    "\n",
    "    df_events_list = funcs[1](df_events_list, dm_model.all_ind_pars)\n",
    "    \n",
    "    frame_times = t_r * (np.arange(n_scans) + t_r/2)\n",
    "    \n",
    "    df_events = pd.concat(df_events_list)\n",
    "    signals = []\n",
    "    for name0, group0 in tqdm(df_events.groupby(['subjID'])):\n",
    "        signal_subject = []\n",
    "        for name1, group1 in group0.groupby(['run']):\n",
    "            exp_condition = np.array(group1[['onset', 'duration', 'modulation']]).T\n",
    "\n",
    "            signal, name = compute_regressor(\n",
    "                exp_condition=exp_condition,\n",
    "                hrf_model=hrf_model,\n",
    "                frame_times=frame_times)\n",
    "            signal_subject.append(signal)\n",
    "        \n",
    "        signal_subject = np.array(signal_subject)\n",
    "        reshape_target = signal_subject.shape\n",
    "        \n",
    "        normalized_signal = minmax_scale(signal_subject.flatten(), axis=0, copy=True)\n",
    "        normalized_signal = normalized_signal.reshape(-1, n_scans, 1)\n",
    "        signals.append(normalized_signal)\n",
    "    signals = np.array(signals)\n",
    "    \n",
    "    if save:\n",
    "        if save_path is None:\n",
    "            sp = Path(layout.derivatives['fMRIPrep'].root) / 'data'\n",
    "        else:\n",
    "            sp = Path(save_path)\n",
    "            \n",
    "        if not sp.exists():\n",
    "            sp.mkdir()\n",
    "        \n",
    "        if single_file:\n",
    "            np.save(sp / 'y.npy', signals)\n",
    "        else:\n",
    "            for i in range(signals.shape[0]):\n",
    "                np.save(sp / f'y_{i+1}.npy', signals[i])\n",
    "    \n",
    "    return dm_model, df_events, signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 88 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 88 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel: cached-ra_prospect-pystan_2.19.1.1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmpqezyweln/output.csv`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model  = ra_prospect\n",
      "Data   = <pandas.DataFrame object>\n",
      "\n",
      "Details:\n",
      " # of chains                    = 4\n",
      " # of cores used                = 4\n",
      " # of MCMC samples (per chain)  = 4000\n",
      " # of burn-in samples           = 1000\n",
      " # of subjects                  = 16\n",
      " # of (max) trials per subject  = 256\n",
      "\n",
      "Using cached StanModel: cached-ra_prospect-pystan_2.19.1.1.pkl\n"
     ]
    }
   ],
   "source": [
    "a, b, y = prepare_events(args['root'], funcs, dm_model='ra_prospect', ncore=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from tensorflow.keras import Sequential, layers, losses, optimizers, datasets\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(X.shape[0])\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # for printing the statistics of the function\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        \n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):  # index : batch no.\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        niis = [X[i] for i  in indexes]\n",
    "        targets = [y[i] for i in indexes]\n",
    "        niis = np.array(niis)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        return niis, targets  # return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "lambda_par = 1\n",
    "repeat_N = 100\n",
    "batch_size = 64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(48 * 240, -1)\n",
    "y = y.reshape(48 * 240, -1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = []\n",
    "\n",
    "for repeat_i in range(repeat_N):\n",
    "    ids = np.arange(X.shape[0])\n",
    "    train_ids, test_ids = train_test_split(ids, test_size=0.2, random_state=repeat_i)\n",
    "    train_steps = len(train_ids) // batch_size\n",
    "    valid_steps = len(test_ids) // batch_size\n",
    "\n",
    "    X_train = X[train_ids]\n",
    "    y_train = y[train_ids]\n",
    "    X_test = X[test_ids]\n",
    "    y_test = y[test_ids]\n",
    "    \n",
    "    train_generator = DataGenerator(X_train, y_train, batch_size, shuffle=True)\n",
    "    valid_generator = DataGenerator(X_test, y_test, batch_size, shuffle=False)\n",
    "    \n",
    "    bst_model_path = f'ml_models/{repeat_i}_best_elasticnet_alpha{alpha:0.3f}_lambda{lambda_par:0.2f}.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True,monitor='val_loss',mode='min',)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='linear', input_shape=(X.shape[-1],),\n",
    "                    use_bias=True,\n",
    "                    kernel_regularizer=l1_l2(lambda_par*alpha,lambda_par*(1-alpha)/2),)) \n",
    "    model.compile(loss='mse', optimizer='adam',)\n",
    "    \n",
    "    model.fit_generator(generator=train_generator, validation_data=valid_generator,\n",
    "                        steps_per_epoch=train_steps, validation_steps=valid_steps,\n",
    "                        epochs=epochs,\n",
    "                        verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=5), model_checkpoint])\n",
    "    model.load_weights(bst_model_path)\n",
    "        \n",
    "    pred = model(X_test).numpy()\n",
    "    mse = ((pred-y_test)**2).mean()\n",
    "    print(f'INFO [{repeat_i+1}/{repeat_N}] - mse: {mse:.03f}')\n",
    "    coeff = model.layers[0].get_weights()[0]  \n",
    "    coeffs.append(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6810, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(coeffs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO [0.001]: done! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mybirth0407/anaconda3/envs/mva/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mni = load_mni152_brain_mask()\n",
    "\n",
    "t_val = ttest_1samp(coeffs,0).statistic\n",
    "\n",
    "t_map = np.zeros(masked_data.get_fdata().flatten().shape[0])\n",
    "\n",
    "for i,v in zip(np.nonzero(masked_data.get_fdata().flatten())[0] ,t_val):\n",
    "    t_map[i] = v\n",
    "    \n",
    "t_map = t_map.reshape(masked_data.shape)\n",
    "\n",
    "t_map = resize(t_map,(91,109,91))\n",
    "\n",
    "t_map *= mni.get_data()\n",
    "nii_i = nib.Nifti1Image(t_map, affine=mni.affine)\n",
    "\n",
    "nii_i.to_filename(f'elasticnet_keras_masked_alpha{alpha:0.3f}_lambda{lambda_par:0.4f}.nii')\n",
    "\n",
    "print(f'INFO [{alpha}]: done! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (MVPA)",
   "language": "python",
   "name": "mvpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
