{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import bids\n",
    "from bids import BIDSLayout, BIDSValidator\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nilearn as nil\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.image import resample_to_img, resample_img\n",
    "from nilearn.datasets import load_mni152_template, load_mni152_brain_mask\n",
    "from nilearn.glm.first_level.hemodynamic_models import compute_regressor\n",
    "import nibabel as nib\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from scipy.stats import ttest_1samp, ttest_ind, t, sem\n",
    "import hbayesdm.models\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.config.set_option('extension_initial_dot', True)\n",
    "\n",
    "args = {\n",
    "    'root': '/data2/project_model_based_fmri/ds000005/',\n",
    "    'mask_path': '/data2/project_model_based_fmri/ds000005/masking',\n",
    "    'save_path': '/data2/project_model_based_fmri/ds000005/data',\n",
    "    \n",
    "    'dm_model': 'ra_prospect',\n",
    "    'hrf_model': 'glover',\n",
    "    'ncore': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2pindex(array, p_value=0.05, flatten=False):\n",
    "    confidence = 1 - p_value\n",
    "    flattened_array = array.flatten()\n",
    "    \n",
    "    n = len(flattened_array)\n",
    "    m = np.mean(flattened_array)\n",
    "    std_err = sem(flattened_array)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    end = m + h\n",
    "    \n",
    "    ret = (flattened_array >= end) if flatten is True else (array >= end)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_masking(mask_path, p_value, zoom, smoothing_fwhm, interpolation_func, flatten=False):\n",
    "    if mask_path is None:\n",
    "        assert (mask_path is None)\n",
    "    elif type(mask_path) is str:\n",
    "        mask_files = [mask_path]\n",
    "    else:\n",
    "        if type(mask_path) is not type(Path()):\n",
    "            mask_path = Path(mask_path)\n",
    "        mask_files = [file for file in mask_path.glob('*.nii.gz')]\n",
    "\n",
    "    mni152_mask = load_mni152_brain_mask()\n",
    "\n",
    "    m = array2pindex(nib.load(mask_files[0]).get_fdata(), p_value, flatten)\n",
    "\n",
    "    if len(mask_files) == 1:\n",
    "        return m\n",
    "    else:\n",
    "        for i in range(1, len(mask_files)):\n",
    "            m |= array2pindex(nib.load(mask_files[i]).get_fdata(), p_value, flatten)\n",
    "\n",
    "    m = block_reduce(m, zoom, interpolation_func)\n",
    "    m = 1 * (m > 0)\n",
    "\n",
    "    m_true = np.array([i for i, v in enumerate(m.flatten()) if v != 0])\n",
    "    masked_data = nib.Nifti1Image(m, affine=mni152_mask.affine)\n",
    "    masker = NiftiMasker(mask_img=masked_data,\n",
    "                         standardize=True,\n",
    "                         smoothing_fwhm=smoothing_fwhm)\n",
    "\n",
    "    return masked_data, masker, m_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(params):\n",
    "    image_filepath, confounds, motion_confounds, masker, masked_data, num = params\n",
    "    \n",
    "    if confounds is not None:\n",
    "        confounds = pd.read_table(confounds, sep='\\t')\n",
    "        confounds = confounds[motion_confounds]\n",
    "        confounds = confounds.to_numpy()\n",
    "\n",
    "    fmri_masked = resample_to_img(image_filepath, masked_data)\n",
    "    fmri_masked = masker.fit_transform(fmri_masked, confounds=confounds)\n",
    "    \n",
    "    return fmri_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bids_preprocess(root, mask_path, \n",
    "                    save_path=None,\n",
    "                    single_file=False,\n",
    "                    zoom=(1, 1, 1),\n",
    "                    smoothing_fwhm=6,\n",
    "                    interpolation_func=np.mean,\n",
    "                    motion_confounds=['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z'],\n",
    "                    p_value=0.05,\n",
    "                    task_name='task-zero',\n",
    "                    ncore=os.cpu_count()):\n",
    "\n",
    "    masked_data, masker, m_true = custom_masking(mask_path, p_value, zoom, smoothing_fwhm, interpolation_func)\n",
    "    print(masked_data.get_fdata().shape)\n",
    "    print(m_true.shape)\n",
    "    print('...preprocessing')\n",
    "    \n",
    "    print('...loading and validating BIDS')\n",
    "    layout = BIDSLayout(root, derivatives=True)\n",
    "    nii_layout = layout.derivatives['fMRIPrep'].get(return_type='file', suffix='bold', extension='nii.gz')\n",
    "    reg_layout = layout.derivatives['fMRIPrep'].get(return_type='file', suffix='regressors', extension='tsv')\n",
    "    print('...done!')\n",
    "    \n",
    "    print('...image processing using %s cores' % ncore)\n",
    "    params = [[z[0], z[1], motion_confounds, masker, masked_data, i]\n",
    "            for i, z in enumerate(zip(nii_layout, reg_layout))]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        X = np.array(list(executor.map(image_preprocess, params)))\n",
    "    print('...done!')\n",
    "\n",
    "    if save_path is not None:\n",
    "        sp = Path(args['save_path'])\n",
    "        if not sp.exists():\n",
    "            sp.mkdir()\n",
    "        \n",
    "        if single_file:\n",
    "            np.save(sp / 'X.npy', X)\n",
    "        else:\n",
    "            for i in range(X.shape[0]):\n",
    "                np.save(sp / f'X_{i+1}.npy', X[i])\n",
    "        np.save(sp / 'restore_map.npy', m_true)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return X, m_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 55, 46)\n",
      "(6810,)\n",
      "...preprocessing\n",
      "...loading and validating BIDS\n",
      "...done!\n",
      "...image processing using 88 cores\n",
      "...done!\n",
      "time elapsed: 56.7156138420105\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "X, m_true = bids_preprocess(Path(args['root']), Path(args['mask_path']), Path(args['save_path']), single_file=True, zoom=(2, 2, 2))\n",
    "e = time.time()\n",
    "print('time elapsed:', e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 240, 6810), (6810,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, m_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events(params, df_events, df_events_info):\n",
    "    for i in range(len(df_events)):\n",
    "        df_events[i]['run'] = df_events_info[i]['run']\n",
    "        df_events[i]['subjID'] = df_events_info[i]['subject']\n",
    "        df_events[i]['gamble'] = df_events[i]['respcat'].apply(lambda x: 1 if x == 1 else 0)\n",
    "        df_events[i]['cert'] = 0 # certain..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_modulation(params, df_events, latent_params):\n",
    "    # postprep\n",
    "    for i in range(len(df_events)):\n",
    "        idx = df_events[i].iloc[0]['subjID']\n",
    "        df_events[i]['rho'] = latent_params.loc[idx]['rho']\n",
    "        df_events[i]['lambda'] = latent_params.loc[idx]['lambda']\n",
    "        df_events[i]['modulation'] = \\\n",
    "            (df_events[i]['gain'] ** df_events[i]['rho']) \\\n",
    "            - (df_events[i]['lambda'] * (df_events[i]['loss'] ** df_events[i]['rho']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [preprocess_events, calculate_modulation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_events(params, funcs):\n",
    "    layout = BIDSLayout(params['root'], derivatives=True)\n",
    "    t_r = layout.get_tr()\n",
    "    events = layout.get(suffix='events', extension='tsv')\n",
    "    image_sample = nib.load(\n",
    "        layout.derivatives['fMRIPrep'].get(\n",
    "            return_type='file',\n",
    "            suffix='bold',\n",
    "            extension='nii.gz')[0]\n",
    "    )\n",
    "    n_scans = image_sample.shape[-1]\n",
    "    \n",
    "    df_events_list = [event.get_df() for event in events]\n",
    "    df_events_info = [event.get_entities() for event in events]\n",
    "    \n",
    "    if len(df_events_list) != len(df_events_info):\n",
    "        assert()\n",
    "    \n",
    "    funcs[0](params, df_events_list, df_events_info)\n",
    "    \n",
    "    ncore = os.cpu_count() - 1\n",
    "    if 'ncore' in params.keys():\n",
    "        ncore = params['ncore']\n",
    "        \n",
    "    dm_model = getattr(hbayesdm.models, params['dm_model'])(\n",
    "        data=pd.concat(df_events_list), ncore=ncore)\n",
    "\n",
    "    funcs[1](params, df_events_list, dm_model.all_ind_pars)\n",
    "    \n",
    "    frame_times = t_r * (np.arange(n_scans) + t_r/2)\n",
    "    \n",
    "    df_events = pd.concat(df_events_list)\n",
    "    signals = []\n",
    "    for name0, group0 in tqdm(df_events.groupby(['subjID'])):\n",
    "        signal_subject = []\n",
    "        for name1, group1 in df_events.groupby(['run']):\n",
    "            exp_condition = np.array(group1[['onset', 'duration', 'modulation']]).T\n",
    "\n",
    "            signal, name = compute_regressor(\n",
    "                exp_condition=exp_condition,\n",
    "                hrf_model=params['hrf_model'],\n",
    "                frame_times=frame_times)\n",
    "            signal_subject.append(signal)\n",
    "        \n",
    "        signal_subject = np.array(signal_subject)\n",
    "        reshape_target = signal_subject.shape\n",
    "        \n",
    "        normalized_signal = minmax_scale(signal_subject.flatten(), axis=0, copy=True)\n",
    "        normalized_signal = normalized_signal.reshape(-1, n_scans, 1)\n",
    "        signals.append(normalized_signal)\n",
    "    signals = np.array(signals)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        sp = Path(args['save_path'])\n",
    "        if not sp.exists():\n",
    "            sp.mkdir()\n",
    "        np.save(sp / 'y.npy', X)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return dm_model, df_events, signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: detected 88 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 88 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel: cached-ra_prospect-pystan_2.19.1.1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmpishird91/output.csv`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model  = ra_prospect\n",
      "Data   = <pandas.DataFrame object>\n",
      "\n",
      "Details:\n",
      " # of chains                    = 4\n",
      " # of cores used                = 32\n",
      " # of MCMC samples (per chain)  = 4000\n",
      " # of burn-in samples           = 1000\n",
      " # of subjects                  = 16\n",
      " # of (max) trials per subject  = 256\n",
      "\n",
      "Using cached StanModel: cached-ra_prospect-pystan_2.19.1.1.pkl\n"
     ]
    }
   ],
   "source": [
    "a, b, y = prepare_events(args, funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save\n",
    "with open('data2.pkl', 'wb') as f:\n",
    "    pickle.dump([X, y.reshape(48, 240, 1)], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 240, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from tensorflow.keras import Sequential, layers, losses, optimizers, datasets\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(X.shape[0])\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # for printing the statistics of the function\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        \n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):  # index : batch no.\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        niis = [X[i] for i  in indexes]\n",
    "        targets = [y[i] for i in indexes]\n",
    "        niis = np.array(niis)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        return niis, targets  # return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "lambda_par = 1\n",
    "repeat_N = 100\n",
    "batch_size = 64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11520, 32513), (48, 240, 1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 32513) (11520, 1)\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape(-1, 32513)\n",
    "y = c.reshape(-1, 1)\n",
    "print(X1.shape, y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'repeat_N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c676a882fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrepeat_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepeat_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repeat_N' is not defined"
     ]
    }
   ],
   "source": [
    "coeffs = []\n",
    "\n",
    "for repeat_i in range(repeat_N):\n",
    "    ids = np.arange(X.shape[0])\n",
    "    train_ids, test_ids = train_test_split(ids, test_size=0.2, random_state=repeat_i)\n",
    "    train_steps = len(train_ids) // batch_size\n",
    "    valid_steps = len(test_ids) // batch_size\n",
    "\n",
    "    X_train = X[train_ids]\n",
    "    y_train = y[train_ids]\n",
    "    X_test = X[test_ids]\n",
    "    y_test = y[test_ids]\n",
    "    \n",
    "    train_generator = DataGenerator(X_train, y_train, batch_size, shuffle=True)\n",
    "    valid_generator = DataGenerator(X_test, y_test, batch_size, shuffle=False)\n",
    "    \n",
    "    bst_model_path = f'{repeat_i}_best_elasticnet_alpha{alpha:0.3f}_lambda{lambda_par:0.2f}.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True,monitor='val_loss',mode='min',)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='linear', input_shape=(X.shape[-1],),\n",
    "                    use_bias=True,\n",
    "                    kernel_regularizer=l1_l2(lambda_par*alpha,lambda_par*(1-alpha)/2),)) \n",
    "    model.compile(loss='mse', optimizer='adam',)\n",
    "    \n",
    "    model.fit_generator(generator=train_generator, validation_data=valid_generator,\n",
    "                        steps_per_epoch=train_steps, validation_steps=valid_steps,\n",
    "                        e pochs=epochs,\n",
    "                        verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=5), model_checkpoint])\n",
    "    model.load_weights(bst_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    pred = model(X_test).numpy()\n",
    "    mse = ((pred-y_test)**2).mean()\n",
    "    print(f'INFO [{repeat_i+1}/{repeat_N}] - mse: {mse:.03f}')\n",
    "    coeff = model.layers[0].get_weights()[0]  \n",
    "    coeffs.append(coeff)\n",
    "\n",
    "coeffs = np.array(coeffs)\n",
    "\n",
    "t_val = ttest_1samp(coeffs,0).statistic.reshape((23, 28, 23))\n",
    "t_val = resize(t_val,(91,109,91))\n",
    "\n",
    "t_val *= mask.get_data()\n",
    "nii_i = nib.Nifti1Image(t_val, affine=mask.affine)\n",
    "\n",
    "nii_i.to_filename(f'elasticnet_keras_alpha{alpha:0.3f}_lambda{lambda_par:0.2f}.nii')\n",
    "\n",
    "print(f'INFO [{alpha}]: done! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (MVPA)",
   "language": "python",
   "name": "mvpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
